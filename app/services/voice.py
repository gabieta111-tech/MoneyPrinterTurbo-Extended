import asyncio
import os
import re
from datetime import datetime
from typing import Union
from xml.sax.saxutils import unescape

# Suppress warnings and handle CUDA library conflicts
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pyannote")
warnings.filterwarnings("ignore", message=".*audio is shorter than 30s.*")
os.environ["PYANNOTE_CACHE"] = "/tmp/pyannote"
# Handle CUDA library conflicts gracefully
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"  # Reduce CUDA memory issues
# Set environment variable to handle cuDNN version mismatches
os.environ["CUDNN_LOGINFO_DBG"] = "0"  # Suppress cuDNN debug info

import edge_tts
import requests
from edge_tts import SubMaker
try:
    from edge_tts.submaker import mktimestamp
except ImportError:
    # Fallback for newer edge_tts versions
    def mktimestamp(offset):
        return str(offset)
from loguru import logger
from moviepy.video.tools import subtitles

from app.config import config
from app.utils import utils

# Import Chatterbox TTS and WhisperX if available
try:
    from chatterbox.tts import ChatterboxTTS
    import whisperx
    import torch
    import torchaudio
    CHATTERBOX_AVAILABLE = True
    logger.info("Chatterbox TTS and WhisperX are available")
except ImportError as e:
    CHATTERBOX_AVAILABLE = False
    logger.warning(f"Chatterbox TTS or WhisperX not available: {e}")

# Global Chatterbox model instance
chatterbox_model = None
whisperx_model = None

# Import Qwen TTS if available
try:
    from qwen_tts.model import Qwen3TTSModel
    import torch
    import torchaudio
    QWEN_TTS_AVAILABLE = True
    logger.info("Qwen TTS is available")
except ImportError as e:
    QWEN_TTS_AVAILABLE = False
    logger.warning(f"Qwen TTS not available: {e}")

# Global Qwen model instance
qwen_tts_model = None


def ensure_submaker_compatibility(sub_maker):
    """Ensure SubMaker has required attributes for compatibility with different edge_tts versions"""
    if not hasattr(sub_maker, 'subs'):
        sub_maker.subs = []
    if not hasattr(sub_maker, 'offset'):
        sub_maker.offset = []
    return sub_maker


def get_siliconflow_voices() -> list[str]:
    """
    Ëé∑ÂèñÁ°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥ÂàóË°®

    Returns:
        Â£∞Èü≥ÂàóË°®ÔºåÊ†ºÂºè‰∏∫ ["siliconflow:FunAudioLLM/CosyVoice2-0.5B:alex", ...]
    """
    # Á°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥ÂàóË°®ÂíåÂØπÂ∫îÁöÑÊÄßÂà´ÔºàÁî®‰∫éÊòæÁ§∫Ôºâ
    voices_with_gender = [
        ("FunAudioLLM/CosyVoice2-0.5B", "alex", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "anna", "Female"),
        ("FunAudioLLM/CosyVoice2-0.5B", "bella", "Female"),
        ("FunAudioLLM/CosyVoice2-0.5B", "benjamin", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "charles", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "claire", "Female"),
        ("FunAudioLLM/CosyVoice2-0.5B", "david", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "diana", "Female"),
    ]

    # Ê∑ªÂä†siliconflow:ÂâçÁºÄÔºåÂπ∂Ê†ºÂºèÂåñ‰∏∫ÊòæÁ§∫ÂêçÁß∞
    return [
        f"siliconflow:{model}:{voice}-{gender}"
        for model, voice, gender in voices_with_gender
    ]


def get_chatterbox_voices() -> list[str]:
    """
    Ëé∑ÂèñChatterbox TTSÁöÑÂ£∞Èü≥ÂàóË°®

    Returns:
        Â£∞Èü≥ÂàóË°®ÔºåÊ†ºÂºè‰∏∫ ["chatterbox:default:Default Voice", "chatterbox:clone:Voice Clone", ...]
    """
    if not CHATTERBOX_AVAILABLE:
        return []
    
    voices = [
        "chatterbox:default:Default Voice-Neutral",
        "chatterbox:clone:Voice Clone-Custom"
    ]
    
    # Add reference audio files from reference_audio directory if available
    reference_audio_dir = os.path.join(utils.root_dir(), "reference_audio")
    if os.path.exists(reference_audio_dir):
        for file in os.listdir(reference_audio_dir):
            if file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):
                name = os.path.splitext(file)[0]
                voices.append(f"chatterbox:clone:{name}-Custom")
    
    return voices


def get_qwen_voices() -> list[str]:
    """
    Ëé∑ÂèñQwen TTSÁöÑÂ£∞Èü≥ÂàóË°®
    Always returns voices even if qwen-tts is not installed,
    so the UI shows them. Generation will give a clear error if not installed.
    """
    if not QWEN_TTS_AVAILABLE:
        logger.warning("Qwen TTS package not installed - voices shown but generation will require: pip install qwen-tts")
    
    voices = [
        "qwen:default:Default Voice-Neutral",
        "qwen:clone:Voice Clone-Custom"
    ]
    
    # Add reference audio files from reference_audio directory if available
    reference_audio_dir = os.path.join(utils.root_dir(), "reference_audio")
    if os.path.exists(reference_audio_dir):
        for file in os.listdir(reference_audio_dir):
            if file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):
                name = os.path.splitext(file)[0]
                voices.append(f"qwen:clone:{name}-Custom")
    
    return voices


def get_all_azure_voices(filter_locals=None) -> list[str]:
    azure_voices_str = """
Name: af-ZA-AdriNeural
Gender: Female

Name: af-ZA-WillemNeural
Gender: Male

Name: am-ET-AmehaNeural
Gender: Male

Name: am-ET-MekdesNeural
Gender: Female

Name: ar-AE-FatimaNeural
Gender: Female

Name: ar-AE-HamdanNeural
Gender: Male

Name: ar-BH-AliNeural
Gender: Male

Name: ar-BH-LailaNeural
Gender: Female

Name: ar-DZ-AminaNeural
Gender: Female

Name: ar-DZ-IsmaelNeural
Gender: Male

Name: ar-EG-SalmaNeural
Gender: Female

Name: ar-EG-ShakirNeural
Gender: Male

Name: ar-IQ-BasselNeural
Gender: Male

Name: ar-IQ-RanaNeural
Gender: Female

Name: ar-JO-SanaNeural
Gender: Female

Name: ar-JO-TaimNeural
Gender: Male

Name: ar-KW-FahedNeural
Gender: Male

Name: ar-KW-NouraNeural
Gender: Female

Name: ar-LB-LaylaNeural
Gender: Female

Name: ar-LB-RamiNeural
Gender: Male

Name: ar-LY-ImanNeural
Gender: Female

Name: ar-LY-OmarNeural
Gender: Male

Name: ar-MA-JamalNeural
Gender: Male

Name: ar-MA-MounaNeural
Gender: Female

Name: ar-OM-AbdullahNeural
Gender: Male

Name: ar-OM-AyshaNeural
Gender: Female

Name: ar-QA-AmalNeural
Gender: Female

Name: ar-QA-MoazNeural
Gender: Male

Name: ar-SA-HamedNeural
Gender: Male

Name: ar-SA-ZariyahNeural
Gender: Female

Name: ar-SY-AmanyNeural
Gender: Female

Name: ar-SY-LaithNeural
Gender: Male

Name: ar-TN-HediNeural
Gender: Male

Name: ar-TN-ReemNeural
Gender: Female

Name: ar-YE-MaryamNeural
Gender: Female

Name: ar-YE-SalehNeural
Gender: Male

Name: az-AZ-BabekNeural
Gender: Male

Name: az-AZ-BanuNeural
Gender: Female

Name: bg-BG-BorislavNeural
Gender: Male

Name: bg-BG-KalinaNeural
Gender: Female

Name: bn-BD-NabanitaNeural
Gender: Female

Name: bn-BD-PradeepNeural
Gender: Male

Name: bn-IN-BashkarNeural
Gender: Male

Name: bn-IN-TanishaaNeural
Gender: Female

Name: bs-BA-GoranNeural
Gender: Male

Name: bs-BA-VesnaNeural
Gender: Female

Name: ca-ES-EnricNeural
Gender: Male

Name: ca-ES-JoanaNeural
Gender: Female

Name: cs-CZ-AntoninNeural
Gender: Male

Name: cs-CZ-VlastaNeural
Gender: Female

Name: cy-GB-AledNeural
Gender: Male

Name: cy-GB-NiaNeural
Gender: Female

Name: da-DK-ChristelNeural
Gender: Female

Name: da-DK-JeppeNeural
Gender: Male

Name: de-AT-IngridNeural
Gender: Female

Name: de-AT-JonasNeural
Gender: Male

Name: de-CH-JanNeural
Gender: Male

Name: de-CH-LeniNeural
Gender: Female

Name: de-DE-AmalaNeural
Gender: Female

Name: de-DE-ConradNeural
Gender: Male

Name: de-DE-FlorianMultilingualNeural
Gender: Male

Name: de-DE-KatjaNeural
Gender: Female

Name: de-DE-KillianNeural
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural
Gender: Female

Name: el-GR-AthinaNeural
Gender: Female

Name: el-GR-NestorasNeural
Gender: Male

Name: en-AU-NatashaNeural
Gender: Female

Name: en-AU-WilliamNeural
Gender: Male

Name: en-CA-ClaraNeural
Gender: Female

Name: en-CA-LiamNeural
Gender: Male

Name: en-GB-LibbyNeural
Gender: Female

Name: en-GB-MaisieNeural
Gender: Female

Name: en-GB-RyanNeural
Gender: Male

Name: en-GB-SoniaNeural
Gender: Female

Name: en-GB-ThomasNeural
Gender: Male

Name: en-HK-SamNeural
Gender: Male

Name: en-HK-YanNeural
Gender: Female

Name: en-IE-ConnorNeural
Gender: Male

Name: en-IE-EmilyNeural
Gender: Female

Name: en-IN-NeerjaExpressiveNeural
Gender: Female

Name: en-IN-NeerjaNeural
Gender: Female

Name: en-IN-PrabhatNeural
Gender: Male

Name: en-KE-AsiliaNeural
Gender: Female

Name: en-KE-ChilembaNeural
Gender: Male

Name: en-NG-AbeoNeural
Gender: Male

Name: en-NG-EzinneNeural
Gender: Female

Name: en-NZ-MitchellNeural
Gender: Male

Name: en-NZ-MollyNeural
Gender: Female

Name: en-PH-JamesNeural
Gender: Male

Name: en-PH-RosaNeural
Gender: Female

Name: en-SG-LunaNeural
Gender: Female

Name: en-SG-WayneNeural
Gender: Male

Name: en-TZ-ElimuNeural
Gender: Male

Name: en-TZ-ImaniNeural
Gender: Female

Name: en-US-AnaNeural
Gender: Female

Name: en-US-AndrewMultilingualNeural
Gender: Male

Name: en-US-AndrewNeural
Gender: Male

Name: en-US-AriaNeural
Gender: Female

Name: en-US-AvaMultilingualNeural
Gender: Female

Name: en-US-AvaNeural
Gender: Female

Name: en-US-BrianMultilingualNeural
Gender: Male

Name: en-US-BrianNeural
Gender: Male

Name: en-US-ChristopherNeural
Gender: Male

Name: en-US-EmmaMultilingualNeural
Gender: Female

Name: en-US-EmmaNeural
Gender: Female

Name: en-US-EricNeural
Gender: Male

Name: en-US-GuyNeural
Gender: Male

Name: en-US-JennyNeural
Gender: Female

Name: en-US-MichelleNeural
Gender: Female

Name: en-US-RogerNeural
Gender: Male

Name: en-US-SteffanNeural
Gender: Male

Name: en-ZA-LeahNeural
Gender: Female

Name: en-ZA-LukeNeural
Gender: Male

Name: es-AR-ElenaNeural
Gender: Female

Name: es-AR-TomasNeural
Gender: Male

Name: es-BO-MarceloNeural
Gender: Male

Name: es-BO-SofiaNeural
Gender: Female

Name: es-CL-CatalinaNeural
Gender: Female

Name: es-CL-LorenzoNeural
Gender: Male

Name: es-CO-GonzaloNeural
Gender: Male

Name: es-CO-SalomeNeural
Gender: Female

Name: es-CR-JuanNeural
Gender: Male

Name: es-CR-MariaNeural
Gender: Female

Name: es-CU-BelkysNeural
Gender: Female

Name: es-CU-ManuelNeural
Gender: Male

Name: es-DO-EmilioNeural
Gender: Male

Name: es-DO-RamonaNeural
Gender: Female

Name: es-EC-AndreaNeural
Gender: Female

Name: es-EC-LuisNeural
Gender: Male

Name: es-ES-AlvaroNeural
Gender: Male

Name: es-ES-ElviraNeural
Gender: Female

Name: es-ES-XimenaNeural
Gender: Female

Name: es-GQ-JavierNeural
Gender: Male

Name: es-GQ-TeresaNeural
Gender: Female

Name: es-GT-AndresNeural
Gender: Male

Name: es-GT-MartaNeural
Gender: Female

Name: es-HN-CarlosNeural
Gender: Male

Name: es-HN-KarlaNeural
Gender: Female

Name: es-MX-DaliaNeural
Gender: Female

Name: es-MX-JorgeNeural
Gender: Male

Name: es-NI-FedericoNeural
Gender: Male

Name: es-NI-YolandaNeural
Gender: Female

Name: es-PA-MargaritaNeural
Gender: Female

Name: es-PA-RobertoNeural
Gender: Male

Name: es-PE-AlexNeural
Gender: Male

Name: es-PE-CamilaNeural
Gender: Female

Name: es-PR-KarinaNeural
Gender: Female

Name: es-PR-VictorNeural
Gender: Male

Name: es-PY-MarioNeural
Gender: Male

Name: es-PY-TaniaNeural
Gender: Female

Name: es-SV-LorenaNeural
Gender: Female

Name: es-SV-RodrigoNeural
Gender: Male

Name: es-US-AlonsoNeural
Gender: Male

Name: es-US-PalomaNeural
Gender: Female

Name: es-UY-MateoNeural
Gender: Male

Name: es-UY-ValentinaNeural
Gender: Female

Name: es-VE-PaolaNeural
Gender: Female

Name: es-VE-SebastianNeural
Gender: Male

Name: et-EE-AnuNeural
Gender: Female

Name: et-EE-KertNeural
Gender: Male

Name: fa-IR-DilaraNeural
Gender: Female

Name: fa-IR-FaridNeural
Gender: Male

Name: fi-FI-HarriNeural
Gender: Male

Name: fi-FI-NooraNeural
Gender: Female

Name: fil-PH-AngeloNeural
Gender: Male

Name: fil-PH-BlessicaNeural
Gender: Female

Name: fr-BE-CharlineNeural
Gender: Female

Name: fr-BE-GerardNeural
Gender: Male

Name: fr-CA-AntoineNeural
Gender: Male

Name: fr-CA-JeanNeural
Gender: Male

Name: fr-CA-SylvieNeural
Gender: Female

Name: fr-CA-ThierryNeural
Gender: Male

Name: fr-CH-ArianeNeural
Gender: Female

Name: fr-CH-FabriceNeural
Gender: Male

Name: fr-FR-DeniseNeural
Gender: Female

Name: fr-FR-EloiseNeural
Gender: Female

Name: fr-FR-HenriNeural
Gender: Male

Name: fr-FR-RemyMultilingualNeural
Gender: Male

Name: fr-FR-VivienneMultilingualNeural
Gender: Female

Name: ga-IE-ColmNeural
Gender: Male

Name: ga-IE-OrlaNeural
Gender: Female

Name: gl-ES-RoiNeural
Gender: Male

Name: gl-ES-SabelaNeural
Gender: Female

Name: gu-IN-DhwaniNeural
Gender: Female

Name: gu-IN-NiranjanNeural
Gender: Male

Name: he-IL-AvriNeural
Gender: Male

Name: he-IL-HilaNeural
Gender: Female

Name: hi-IN-MadhurNeural
Gender: Male

Name: hi-IN-SwaraNeural
Gender: Female

Name: hr-HR-GabrijelaNeural
Gender: Female

Name: hr-HR-SreckoNeural
Gender: Male

Name: hu-HU-NoemiNeural
Gender: Female

Name: hu-HU-TamasNeural
Gender: Male

Name: id-ID-ArdiNeural
Gender: Male

Name: id-ID-GadisNeural
Gender: Female

Name: is-IS-GudrunNeural
Gender: Female

Name: is-IS-GunnarNeural
Gender: Male

Name: it-IT-DiegoNeural
Gender: Male

Name: it-IT-ElsaNeural
Gender: Female

Name: it-IT-GiuseppeMultilingualNeural
Gender: Male

Name: it-IT-IsabellaNeural
Gender: Female

Name: iu-Cans-CA-SiqiniqNeural
Gender: Female

Name: iu-Cans-CA-TaqqiqNeural
Gender: Male

Name: iu-Latn-CA-SiqiniqNeural
Gender: Female

Name: iu-Latn-CA-TaqqiqNeural
Gender: Male

Name: ja-JP-KeitaNeural
Gender: Male

Name: ja-JP-NanamiNeural
Gender: Female

Name: jv-ID-DimasNeural
Gender: Male

Name: jv-ID-SitiNeural
Gender: Female

Name: ka-GE-EkaNeural
Gender: Female

Name: ka-GE-GiorgiNeural
Gender: Male

Name: kk-KZ-AigulNeural
Gender: Female

Name: kk-KZ-DauletNeural
Gender: Male

Name: km-KH-PisethNeural
Gender: Male

Name: km-KH-SreymomNeural
Gender: Female

Name: kn-IN-GaganNeural
Gender: Male

Name: kn-IN-SapnaNeural
Gender: Female

Name: ko-KR-HyunsuMultilingualNeural
Gender: Male

Name: ko-KR-InJoonNeural
Gender: Male

Name: ko-KR-SunHiNeural
Gender: Female

Name: lo-LA-ChanthavongNeural
Gender: Male

Name: lo-LA-KeomanyNeural
Gender: Female

Name: lt-LT-LeonasNeural
Gender: Male

Name: lt-LT-OnaNeural
Gender: Female

Name: lv-LV-EveritaNeural
Gender: Female

Name: lv-LV-NilsNeural
Gender: Male

Name: mk-MK-AleksandarNeural
Gender: Male

Name: mk-MK-MarijaNeural
Gender: Female

Name: ml-IN-MidhunNeural
Gender: Male

Name: ml-IN-SobhanaNeural
Gender: Female

Name: mn-MN-BataaNeural
Gender: Male

Name: mn-MN-YesuiNeural
Gender: Female

Name: mr-IN-AarohiNeural
Gender: Female

Name: mr-IN-ManoharNeural
Gender: Male

Name: ms-MY-OsmanNeural
Gender: Male

Name: ms-MY-YasminNeural
Gender: Female

Name: mt-MT-GraceNeural
Gender: Female

Name: mt-MT-JosephNeural
Gender: Male

Name: my-MM-NilarNeural
Gender: Female

Name: my-MM-ThihaNeural
Gender: Male

Name: nb-NO-FinnNeural
Gender: Male

Name: nb-NO-PernilleNeural
Gender: Female

Name: ne-NP-HemkalaNeural
Gender: Female

Name: ne-NP-SagarNeural
Gender: Male

Name: nl-BE-ArnaudNeural
Gender: Male

Name: nl-BE-DenaNeural
Gender: Female

Name: nl-NL-ColetteNeural
Gender: Female

Name: nl-NL-FennaNeural
Gender: Female

Name: nl-NL-MaartenNeural
Gender: Male

Name: pl-PL-MarekNeural
Gender: Male

Name: pl-PL-ZofiaNeural
Gender: Female

Name: ps-AF-GulNawazNeural
Gender: Male

Name: ps-AF-LatifaNeural
Gender: Female

Name: pt-BR-AntonioNeural
Gender: Male

Name: pt-BR-FranciscaNeural
Gender: Female

Name: pt-BR-ThalitaMultilingualNeural
Gender: Female

Name: pt-PT-DuarteNeural
Gender: Male

Name: pt-PT-RaquelNeural
Gender: Female

Name: ro-RO-AlinaNeural
Gender: Female

Name: ro-RO-EmilNeural
Gender: Male

Name: ru-RU-DmitryNeural
Gender: Male

Name: ru-RU-SvetlanaNeural
Gender: Female

Name: si-LK-SameeraNeural
Gender: Male

Name: si-LK-ThiliniNeural
Gender: Female

Name: sk-SK-LukasNeural
Gender: Male

Name: sk-SK-ViktoriaNeural
Gender: Female

Name: sl-SI-PetraNeural
Gender: Female

Name: sl-SI-RokNeural
Gender: Male

Name: so-SO-MuuseNeural
Gender: Male

Name: so-SO-UbaxNeural
Gender: Female

Name: sq-AL-AnilaNeural
Gender: Female

Name: sq-AL-IlirNeural
Gender: Male

Name: sr-RS-NicholasNeural
Gender: Male

Name: sr-RS-SophieNeural
Gender: Female

Name: su-ID-JajangNeural
Gender: Male

Name: su-ID-TutiNeural
Gender: Female

Name: sv-SE-MattiasNeural
Gender: Male

Name: sv-SE-SofieNeural
Gender: Female

Name: sw-KE-RafikiNeural
Gender: Male

Name: sw-KE-ZuriNeural
Gender: Female

Name: sw-TZ-DaudiNeural
Gender: Male

Name: sw-TZ-RehemaNeural
Gender: Female

Name: ta-IN-PallaviNeural
Gender: Female

Name: ta-IN-ValluvarNeural
Gender: Male

Name: ta-LK-KumarNeural
Gender: Male

Name: ta-LK-SaranyaNeural
Gender: Female

Name: ta-MY-KaniNeural
Gender: Female

Name: ta-MY-SuryaNeural
Gender: Male

Name: ta-SG-AnbuNeural
Gender: Male

Name: ta-SG-VenbaNeural
Gender: Female

Name: te-IN-MohanNeural
Gender: Male

Name: te-IN-ShrutiNeural
Gender: Female

Name: th-TH-NiwatNeural
Gender: Male

Name: th-TH-PremwadeeNeural
Gender: Female

Name: tr-TR-AhmetNeural
Gender: Male

Name: tr-TR-EmelNeural
Gender: Female

Name: uk-UA-OstapNeural
Gender: Male

Name: uk-UA-PolinaNeural
Gender: Female

Name: ur-IN-GulNeural
Gender: Female

Name: ur-IN-SalmanNeural
Gender: Male

Name: ur-PK-AsadNeural
Gender: Male

Name: ur-PK-UzmaNeural
Gender: Female

Name: uz-UZ-MadinaNeural
Gender: Female

Name: uz-UZ-SardorNeural
Gender: Male

Name: vi-VN-HoaiMyNeural
Gender: Female

Name: vi-VN-NamMinhNeural
Gender: Male

Name: zh-CN-XiaoxiaoNeural
Gender: Female

Name: zh-CN-XiaoyiNeural
Gender: Female

Name: zh-CN-YunjianNeural
Gender: Male

Name: zh-CN-YunxiNeural
Gender: Male

Name: zh-CN-YunxiaNeural
Gender: Male

Name: zh-CN-YunyangNeural
Gender: Male

Name: zh-CN-liaoning-XiaobeiNeural
Gender: Female

Name: zh-CN-shaanxi-XiaoniNeural
Gender: Female

Name: zh-HK-HiuGaaiNeural
Gender: Female

Name: zh-HK-HiuMaanNeural
Gender: Female

Name: zh-HK-WanLungNeural
Gender: Male

Name: zh-TW-HsiaoChenNeural
Gender: Female

Name: zh-TW-HsiaoYuNeural
Gender: Female

Name: zh-TW-YunJheNeural
Gender: Male

Name: zu-ZA-ThandoNeural
Gender: Female

Name: zu-ZA-ThembaNeural
Gender: Male


Name: en-US-AvaMultilingualNeural-V2
Gender: Female

Name: en-US-AndrewMultilingualNeural-V2
Gender: Male

Name: en-US-EmmaMultilingualNeural-V2
Gender: Female

Name: en-US-BrianMultilingualNeural-V2
Gender: Male

Name: de-DE-FlorianMultilingualNeural-V2
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural-V2
Gender: Female

Name: fr-FR-RemyMultilingualNeural-V2
Gender: Male

Name: fr-FR-VivienneMultilingualNeural-V2
Gender: Female

Name: zh-CN-XiaoxiaoMultilingualNeural-V2
Gender: Female
    """.strip()
    voices = []
    # ÂÆö‰πâÊ≠£ÂàôË°®ËææÂºèÊ®°ÂºèÔºåÁî®‰∫éÂåπÈÖç Name Âíå Gender Ë°å
    pattern = re.compile(r"Name:\s*(.+)\s*Gender:\s*(.+)\s*", re.MULTILINE)
    # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊü•ÊâæÊâÄÊúâÂåπÈÖçÈ°π
    matches = pattern.findall(azure_voices_str)

    for name, gender in matches:
        # Â∫îÁî®ËøáÊª§Êù°‰ª∂
        if filter_locals and any(
            name.lower().startswith(fl.lower()) for fl in filter_locals
        ):
            voices.append(f"{name}-{gender}")
        elif not filter_locals:
            voices.append(f"{name}-{gender}")

    voices.sort()
    return voices


def parse_voice_name(name: str):
    # zh-CN-XiaoyiNeural-Female
    # zh-CN-YunxiNeural-Male
    # zh-CN-XiaoxiaoMultilingualNeural-V2-Female
    name = name.replace("-Female", "").replace("-Male", "").strip()
    return name


def is_azure_v2_voice(voice_name: str):
    voice_name = parse_voice_name(voice_name)
    if voice_name.endswith("-V2"):
        return voice_name.replace("-V2", "").strip()
    return ""


def is_siliconflow_voice(voice_name: str):
    """Ê£ÄÊü•ÊòØÂê¶ÊòØÁ°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥"""
    return voice_name.startswith("siliconflow:")


def is_chatterbox_voice(voice_name: str):
    """Ê£ÄÊü•ÊòØÂê¶ÊòØChatterboxÁöÑÂ£∞Èü≥"""
    return voice_name.startswith("chatterbox:")


def is_qwen_voice(voice_name: str):
    """Ê£ÄÊü•ÊòØÂê¶ÊòØQwenÁöÑÂ£∞Èü≥"""
    return voice_name.startswith("qwen:")


def tts(
    text: str,
    voice_name: str,
    voice_rate: float,
    voice_file: str,
    voice_volume: float = 1.0,
) -> Union[SubMaker, None]:
    if is_azure_v2_voice(voice_name):
        return azure_tts_v2(text, voice_name, voice_file)
    elif is_siliconflow_voice(voice_name):
        # ‰ªévoice_name‰∏≠ÊèêÂèñÊ®°ÂûãÂíåÂ£∞Èü≥
        # Ê†ºÂºè: siliconflow:model:voice-Gender
        parts = voice_name.split(":")
        if len(parts) >= 3:
            model = parts[1]
            # ÁßªÈô§ÊÄßÂà´ÂêéÁºÄÔºå‰æãÂ¶Ç "alex-Male" -> "alex"
            voice_with_gender = parts[2]
            voice = voice_with_gender.split("-")[0]
            # ÊûÑÂª∫ÂÆåÊï¥ÁöÑvoiceÂèÇÊï∞ÔºåÊ†ºÂºè‰∏∫ "model:voice"
            full_voice = f"{model}:{voice}"
            return siliconflow_tts(
                text, model, full_voice, voice_rate, voice_file, voice_volume
            )
        else:
            logger.error(f"Invalid siliconflow voice name format: {voice_name}")
            return None
    elif is_chatterbox_voice(voice_name):
        # Chatterbox TTS with WhisperX timestamps
        # Ê†ºÂºè: chatterbox:type:name-Gender
        return chatterbox_tts(text, voice_name, voice_rate, voice_file, voice_volume)
    elif is_qwen_voice(voice_name):
        # Qwen TTS
        return qwen_tts(text, voice_name, voice_rate, voice_file, voice_volume)
    return azure_tts_v1(text, voice_name, voice_rate, voice_file)


def convert_rate_to_percent(rate: float) -> str:
    if rate == 1.0:
        return "+0%"
    percent = round((rate - 1.0) * 100)
    if percent > 0:
        return f"+{percent}%"
    else:
        return f"{percent}%"


def azure_tts_v1(
    text: str, voice_name: str, voice_rate: float, voice_file: str
) -> Union[SubMaker, None]:
    voice_name = parse_voice_name(voice_name)
    text = text.strip()
    rate_str = convert_rate_to_percent(voice_rate)
    for i in range(3):
        try:
            logger.info(f"start, voice name: {voice_name}, try: {i + 1}")

            async def _do() -> SubMaker:
                communicate = edge_tts.Communicate(text, voice_name, rate=rate_str)
                sub_maker = ensure_submaker_compatibility(edge_tts.SubMaker())
                with open(voice_file, "wb") as file:
                    async for chunk in communicate.stream():
                        if chunk["type"] == "audio":
                            file.write(chunk["data"])
                        elif chunk["type"] == "WordBoundary":
                            sub_maker.subs.append(chunk["text"])
                            sub_maker.offset.append((chunk["offset"], chunk["offset"] + chunk["duration"]))
                return sub_maker

            sub_maker = asyncio.run(_do())
            if not sub_maker or not sub_maker.subs:
                logger.warning("failed, sub_maker is None or sub_maker.subs is None")
                continue

            logger.info(f"completed, output file: {voice_file}")
            return sub_maker
        except Exception as e:
            logger.error(f"failed, error: {str(e)}")
    return None


def siliconflow_tts(
    text: str,
    model: str,
    voice: str,
    voice_rate: float,
    voice_file: str,
    voice_volume: float = 1.0,
) -> Union[SubMaker, None]:
    """
    ‰ΩøÁî®Á°ÖÂü∫ÊµÅÂä®ÁöÑAPIÁîüÊàêËØ≠Èü≥

    Args:
        text: Ë¶ÅËΩ¨Êç¢‰∏∫ËØ≠Èü≥ÁöÑÊñáÊú¨
        model: Ê®°ÂûãÂêçÁß∞ÔºåÂ¶Ç "FunAudioLLM/CosyVoice2-0.5B"
        voice: Â£∞Èü≥ÂêçÁß∞ÔºåÂ¶Ç "FunAudioLLM/CosyVoice2-0.5B:alex"
        voice_rate: ËØ≠Èü≥ÈÄüÂ∫¶ÔºåËåÉÂõ¥[0.25, 4.0]
        voice_file: ËæìÂá∫ÁöÑÈü≥È¢ëÊñá‰ª∂Ë∑ØÂæÑ
        voice_volume: ËØ≠Èü≥Èü≥ÈáèÔºåËåÉÂõ¥[0.6, 5.0]ÔºåÈúÄË¶ÅËΩ¨Êç¢‰∏∫Á°ÖÂü∫ÊµÅÂä®ÁöÑÂ¢ûÁõäËåÉÂõ¥[-10, 10]

    Returns:
        SubMakerÂØπË±°ÊàñNone
    """
    text = text.strip()
    api_key = config.siliconflow.get("api_key", "")

    if not api_key:
        logger.error("SiliconFlow API key is not set")
        return None

    # Â∞Üvoice_volumeËΩ¨Êç¢‰∏∫Á°ÖÂü∫ÊµÅÂä®ÁöÑÂ¢ûÁõäËåÉÂõ¥
    # ÈªòËÆ§voice_volume‰∏∫1.0ÔºåÂØπÂ∫îgain‰∏∫0
    gain = voice_volume - 1.0
    # Á°Æ‰øùgainÂú®[-10, 10]ËåÉÂõ¥ÂÜÖ
    gain = max(-10, min(10, gain))

    url = "https://api.siliconflow.cn/v1/audio/speech"

    payload = {
        "model": model,
        "input": text,
        "voice": voice,
        "response_format": "mp3",
        "sample_rate": 32000,
        "stream": False,
        "speed": voice_rate,
        "gain": gain,
    }

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    for i in range(3):  # Â∞ùËØï3Ê¨°
        try:
            logger.info(
                f"start siliconflow tts, model: {model}, voice: {voice}, try: {i + 1}"
            )

            response = requests.post(url, json=payload, headers=headers)

            if response.status_code == 200:
                # ‰øùÂ≠òÈü≥È¢ëÊñá‰ª∂
                with open(voice_file, "wb") as f:
                    f.write(response.content)

                # ÂàõÂª∫‰∏Ä‰∏™Á©∫ÁöÑSubMakerÂØπË±°
                sub_maker = ensure_submaker_compatibility(SubMaker())

                # Ëé∑ÂèñÈü≥È¢ëÊñá‰ª∂ÁöÑÂÆûÈôÖÈïøÂ∫¶
                try:
                    # Â∞ùËØï‰ΩøÁî®moviepyËé∑ÂèñÈü≥È¢ëÈïøÂ∫¶
                    from moviepy import AudioFileClip

                    audio_clip = AudioFileClip(voice_file)
                    audio_duration = audio_clip.duration
                    audio_clip.close()

                    # Â∞ÜÈü≥È¢ëÈïøÂ∫¶ËΩ¨Êç¢‰∏∫100Á∫≥ÁßíÂçï‰ΩçÔºà‰∏éedge_ttsÂÖºÂÆπÔºâ
                    audio_duration_100ns = int(audio_duration * 10000000)

                    # ‰ΩøÁî®ÊñáÊú¨ÂàÜÂâ≤Êù•ÂàõÂª∫Êõ¥ÂáÜÁ°ÆÁöÑÂ≠óÂπï
                    # Â∞ÜÊñáÊú¨ÊåâÊ†áÁÇπÁ¨¶Âè∑ÂàÜÂâ≤ÊàêÂè•Â≠ê
                    sentences = utils.split_string_by_punctuations(text)

                    if sentences:
                        # ËÆ°ÁÆóÊØè‰∏™Âè•Â≠êÁöÑÂ§ßËá¥Êó∂ÈïøÔºàÊåâÂ≠óÁ¨¶Êï∞ÊØî‰æãÂàÜÈÖçÔºâ
                        total_chars = sum(len(s) for s in sentences)
                        char_duration = (
                            audio_duration_100ns / total_chars if total_chars > 0 else 0
                        )

                        current_offset = 0
                        for sentence in sentences:
                            if not sentence.strip():
                                continue

                            # ËÆ°ÁÆóÂΩìÂâçÂè•Â≠êÁöÑÊó∂Èïø
                            sentence_chars = len(sentence)
                            sentence_duration = int(sentence_chars * char_duration)

                            # Ê∑ªÂä†Âà∞SubMaker
                            sub_maker.subs.append(sentence)
                            sub_maker.offset.append(
                                (current_offset, current_offset + sentence_duration)
                            )

                            # Êõ¥Êñ∞ÂÅèÁßªÈáè
                            current_offset += sentence_duration
                    else:
                        # Â¶ÇÊûúÊó†Ê≥ïÂàÜÂâ≤ÔºåÂàô‰ΩøÁî®Êï¥‰∏™ÊñáÊú¨‰Ωú‰∏∫‰∏Ä‰∏™Â≠óÂπï
                        sub_maker.subs = [text]
                        sub_maker.offset = [(0, audio_duration_100ns)]

                except Exception as e:
                    logger.warning(f"Failed to create accurate subtitles: {str(e)}")
                    # ÂõûÈÄÄÂà∞ÁÆÄÂçïÁöÑÂ≠óÂπï
                    sub_maker.subs = [text]
                    # ‰ΩøÁî®Èü≥È¢ëÊñá‰ª∂ÁöÑÂÆûÈôÖÈïøÂ∫¶ÔºåÂ¶ÇÊûúÊó†Ê≥ïËé∑ÂèñÔºåÂàôÂÅáËÆæ‰∏∫10Áßí
                    sub_maker.offset = [
                        (
                            0,
                            audio_duration_100ns
                            if "audio_duration_100ns" in locals()
                            else 10000000,
                        )
                    ]

                logger.success(f"siliconflow tts succeeded: {voice_file}")
                return sub_maker
            else:
                logger.error(
                    f"siliconflow tts failed with status code {response.status_code}: {response.text}"
                )
        except Exception as e:
            logger.error(f"siliconflow tts failed: {str(e)}")

    return None


def preprocess_text_for_chatterbox(text: str) -> str:
    """
    Preprocess text to improve Chatterbox TTS quality and prevent garbled audio
    
    Fixes common issues:
    - Converts numbers to words
    - Simplifies technical terms
    - Shortens complex sentences
    - Reduces punctuation variety
    - Handles contractions properly
    """
    if not text:
        return text
    
    # Clean and normalize text
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Convert number ranges to words
    number_ranges = {
        r'\b150-300\b': 'one hundred fifty to three hundred',
        r'\b75-150\b': 'seventy five to one hundred fifty',
        r'\b150\b': 'one hundred fifty',
        r'\b300\b': 'three hundred',
        r'\b75\b': 'seventy five',
        r'\b6-7\b': 'six to seven',
        r'\b30\b': 'thirty',
        r'\b5\b': 'five',
        r'\b2\b': 'two'
    }
    
    for pattern, replacement in number_ranges.items():
        text = re.sub(pattern, replacement, text)
    
    # Simplify technical terms
    technical_replacements = {
        r'\bmetabolism\b': 'how your body burns calories',
        r'\bantioxidants\b': 'healthy compounds',
        r'\bquinoa\b': 'healthy grains',
        r'\bcardiovascular\b': 'heart and blood vessel',
        r'\bWorld Health Organization\b': 'health experts',
        r'\bvigorous cardio\b': 'intense exercise',
        r'\bmoderate cardio\b': 'gentle exercise',
        r'\bstrengthening activities\b': 'muscle building exercises',
        r'\bresistance bands\b': 'exercise bands',
        r'\bcomplex carbs\b': 'healthy carbs'
    }
    
    for pattern, replacement in technical_replacements.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Reduce excessive punctuation
    text = re.sub(r'!!+', '!', text)  # Multiple exclamations
    text = re.sub(r'\?\?+', '?', text)  # Multiple questions
    text = re.sub(r'\.\.+', '.', text)  # Multiple periods
    
    # Simplify contractions for better pronunciation
    contractions = {
        r"\byou're\b": 'you are',
        r"\bdon't\b": 'do not',
        r"\blet's\b": 'let us',
        r"\bwhen's\b": 'when is',
        r"\bthat's\b": 'that is',
        r"\bit's\b": 'it is'
    }
    
    for pattern, replacement in contractions.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Break very long sentences at natural pause points
    # Split sentences longer than 120 characters
    sentences = re.split(r'([.!?])', text)
    processed_sentences = []
    
    for i in range(0, len(sentences), 2):
        if i + 1 < len(sentences):
            sentence = sentences[i]
            punctuation = sentences[i + 1]
            
            if len(sentence) > 120:
                # Try to split at commas or other natural breaks
                parts = sentence.split(', ')
                if len(parts) > 1:
                    # Rejoin as separate sentences
                    for j, part in enumerate(parts):
                        if j == len(parts) - 1:
                            processed_sentences.append(part + punctuation)
                        else:
                            processed_sentences.append(part.strip() + '.')
                else:
                    processed_sentences.append(sentence + punctuation)
            else:
                processed_sentences.append(sentence + punctuation)
        else:
            processed_sentences.append(sentences[i])
    
    text = ' '.join(processed_sentences)
    
    # Clean up extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def chunk_text_for_chatterbox(text: str, max_chunk_size: int = 300) -> list:
    """
    Split text into optimal chunks for Chatterbox TTS processing
    
    Args:
        text: Input text to chunk
        max_chunk_size: Maximum characters per chunk
        
    Returns:
        List of text chunks
    """
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    sentences = re.split(r'([.!?])', text)
    current_chunk = ""
    
    for i in range(0, len(sentences), 2):
        if i + 1 < len(sentences):
            sentence = sentences[i].strip()
            punctuation = sentences[i + 1]
            full_sentence = sentence + punctuation
            
            # If adding this sentence would exceed chunk size, start new chunk
            if len(current_chunk) + len(full_sentence) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = full_sentence
            else:
                current_chunk += " " + full_sentence if current_chunk else full_sentence
        else:
            # Handle case where there's a sentence without punctuation at the end
            if sentences[i].strip():
                if len(current_chunk) + len(sentences[i]) > max_chunk_size and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentences[i].strip()
                else:
                    current_chunk += " " + sentences[i].strip() if current_chunk else sentences[i].strip()
    
    # Add the last chunk
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks


def chatterbox_tts(
    text: str,
    voice_name: str,
    voice_rate: float,
    voice_file: str,
    voice_volume: float = 1.0,
) -> Union[SubMaker, None]:
    """
    ‰ΩøÁî®Chatterbox TTS + WhisperXÁîüÊàêËØ≠Èü≥ÂíåÁ≤æÁ°ÆÁöÑÂçïËØçÊó∂Èó¥Êà≥

    Args:
        text: Ë¶ÅËΩ¨Êç¢‰∏∫ËØ≠Èü≥ÁöÑÊñáÊú¨
        voice_name: Â£∞Èü≥ÂêçÁß∞ÔºåÊ†ºÂºè: "chatterbox:type:name-Gender"
        voice_rate: ËØ≠Èü≥ÈÄüÂ∫¶ÔºàÊöÇ‰∏çÊîØÊåÅË∞ÉÊï¥Ôºâ
        voice_file: ËæìÂá∫ÁöÑÈü≥È¢ëÊñá‰ª∂Ë∑ØÂæÑ
        voice_volume: ËØ≠Èü≥Èü≥ÈáèÔºàÊöÇ‰∏çÊîØÊåÅË∞ÉÊï¥Ôºâ

    Returns:
        SubMakerÂØπË±°ÊàñNone
    """
    if not CHATTERBOX_AVAILABLE:
        logger.error("Chatterbox TTS is not available. Please install chatterbox-tts and whisperx.")
        return None

    text = text.strip()
    if not text:
        logger.error("Text is empty")
        return None

    # Preprocess text to improve TTS quality
    original_text = text
    text = preprocess_text_for_chatterbox(text)
    
    # Check if text needs chunking (configurable threshold via CHATTERBOX_CHUNK_THRESHOLD)
    # Higher threshold reduces chunking frequency which can affect speech pacing
    chunk_threshold = int(os.environ.get("CHATTERBOX_CHUNK_THRESHOLD", "600"))
    if len(text) > chunk_threshold:
        logger.warning(f"Text is too long ({len(text)} chars) for single-pass Chatterbox TTS")
        logger.info("Automatically chunking text for better quality...")
        return chatterbox_tts_chunked(text, voice_name, voice_rate, voice_file, voice_volume)
    
    logger.info(f"Chatterbox TTS input: '{text[:100]}...' (original: {len(original_text)} ‚Üí processed: {len(text)} chars)")

    # Ëß£Êûêvoice_name: chatterbox:type:name-Gender
    parts = voice_name.split(":")
    if len(parts) < 3:
        logger.error(f"Invalid Chatterbox voice name format: {voice_name}")
        return None

    voice_type = parts[1]  # "default" or "clone"
    voice_info = parts[2]  # "name-Gender"
    voice_base_name = voice_info.split("-")[0]

    # Ëé∑ÂèñËÆæÂ§á - Use CPU by default to avoid cuDNN version conflicts
    # Set CHATTERBOX_DEVICE=cuda environment variable to force GPU usage
    force_device = os.environ.get("CHATTERBOX_DEVICE", "cpu").lower()
    if force_device == "cuda" and torch.cuda.is_available():
        device = "cuda"
        logger.info(f"Using GPU device: {device} (forced via CHATTERBOX_DEVICE)")
    else:
        device = "cpu"
        logger.info(f"Using CPU device (safe mode - set CHATTERBOX_DEVICE=cuda to use GPU)")

    global chatterbox_model, whisperx_model

    try:
        # 1. Âä†ËΩΩChatterbox TTSÊ®°Âûã
        if chatterbox_model is None:
            logger.info("Loading Chatterbox TTS model...")
            try:
                chatterbox_model = ChatterboxTTS.from_pretrained(device=device)
                logger.info("Chatterbox TTS model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load Chatterbox TTS model: {e}")
                if device == "cuda":
                    logger.info("Falling back to CPU mode...")
                    device = "cpu"
                    chatterbox_model = ChatterboxTTS.from_pretrained(device=device)
                    logger.info("Chatterbox TTS model loaded successfully on CPU")
                else:
                    raise

        # 2. ÁîüÊàêËØ≠Èü≥
        logger.info(f"Generating speech with Chatterbox TTS, type: {voice_type}")
        
        audio_prompt_path = None
        if voice_type == "clone" and voice_base_name != "Voice Clone":
            # Êü•ÊâæÂèÇËÄÉÈü≥È¢ëÊñá‰ª∂
            reference_audio_dir = os.path.join(utils.root_dir(), "reference_audio")
            for ext in ['.wav', '.mp3', '.flac', '.m4a']:
                potential_path = os.path.join(reference_audio_dir, voice_base_name + ext)
                if os.path.exists(potential_path):
                    audio_prompt_path = potential_path
                    break
            
            if audio_prompt_path:
                logger.info(f"Using voice cloning with reference: {audio_prompt_path}")
            else:
                logger.warning(f"Reference audio not found for {voice_base_name}, using default voice")

        # ÁîüÊàêËØ≠Èü≥ (with improved pacing control)
        # Lower cfg_weight for slower, more natural pacing
        # Environment variable CHATTERBOX_CFG_WEIGHT can override (default 0.2 for very slow speech)
        cfg_weight = float(os.environ.get("CHATTERBOX_CFG_WEIGHT", "0.2"))
        logger.info(f"Using cfg_weight={cfg_weight} for speech pacing control")
        
        if audio_prompt_path:
            wav = chatterbox_model.generate(text, audio_prompt_path=audio_prompt_path, cfg_weight=cfg_weight)
        else:
            wav = chatterbox_model.generate(text, cfg_weight=cfg_weight)

        # ‰øùÂ≠ò‰∏∫‰∏¥Êó∂WAVÊñá‰ª∂
        temp_wav_file = voice_file.replace('.mp3', '_temp.wav')
        torchaudio.save(temp_wav_file, wav, 24000)

        # 3. ‰ΩøÁî®WhisperXËé∑ÂèñÁ≤æÁ°ÆÁöÑÂçïËØçÊó∂Èó¥Êà≥
        logger.info("Generating word timestamps with WhisperX")
        
        if whisperx_model is None:
            logger.info("Loading WhisperX model...")
            # Use appropriate compute type for CPU
            compute_type = "int8" if device == "cpu" else "float16"
            try:
                whisperx_model = whisperx.load_model("base", device, compute_type=compute_type)
                logger.info(f"WhisperX model loaded successfully on {device} with {compute_type}")
            except Exception as e:
                logger.error(f"Failed to load WhisperX model on {device}: {e}")
                if device == "cuda":
                    logger.info("Falling back to CPU for WhisperX...")
                    device = "cpu"
                    compute_type = "int8"
                    whisperx_model = whisperx.load_model("base", device, compute_type=compute_type)
                    logger.info(f"WhisperX model loaded successfully on CPU with {compute_type}")
                else:
                    raise

        # ËΩ¨ÂΩïÈü≥È¢ëËé∑ÂèñÂçïËØçÊó∂Èó¥Êà≥
        audio = whisperx.load_audio(temp_wav_file)
        result = whisperx_model.transcribe(audio, batch_size=16)

        # Validate transcription result
        transcription_failed = False
        if not result or "segments" not in result or not result["segments"]:
            logger.warning("WhisperX transcription failed or returned empty result")
            logger.debug(f"WhisperX result: {result}")
            transcription_failed = True
        else:
            # Log transcribed text for validation
            transcribed_text = " ".join([segment.get("text", "") for segment in result["segments"]]).strip()
            logger.info(f"WhisperX transcribed: '{transcribed_text[:100]}...' (length: {len(transcribed_text)} chars)")
            
            # Check if transcription matches input text reasonably well
            text_similarity = len(set(text.lower().split()) & set(transcribed_text.lower().split())) / max(len(text.split()), 1)
            logger.debug(f"Text similarity score: {text_similarity:.2f}")
            
            if text_similarity < 0.3:
                logger.warning(f"Transcription seems inaccurate (similarity: {text_similarity:.2f})")
                if text_similarity < 0.1:
                    logger.error(f"Transcription quality too poor (similarity: {text_similarity:.2f}), falling back to sentence-level timing")
                    transcription_failed = True

        # Âä†ËΩΩÂØπÈΩêÊ®°Âûã (only if transcription is good)
        if not transcription_failed:
            try:
                model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
                result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)
            except Exception as e:
                logger.error(f"WhisperX alignment failed: {e}")
                transcription_failed = True

        # 4. ÂàõÂª∫SubMakerÂπ∂Â°´ÂÖÖÊó∂Èó¥Êà≥
        sub_maker = ensure_submaker_compatibility(SubMaker())
        
        # Process word-level timestamps from WhisperX alignment (only if transcription is good)
        word_count = 0
        if not transcription_failed and "segments" in result and result["segments"]:
            # Debug: Log the WhisperX result structure
            logger.debug(f"WhisperX result keys: {list(result.keys())}")
            logger.debug(f"Number of segments: {len(result['segments'])}")
            if result["segments"]:
                logger.debug(f"First segment keys: {list(result['segments'][0].keys())}")
            
            for segment in result["segments"]:
                # Check if this segment has word-level alignments
                if "words" in segment and segment["words"]:
                    for word_info in segment["words"]:
                        word = word_info.get("word", "").strip()
                        start = word_info.get("start", None)
                        end = word_info.get("end", None)
                        
                        # Skip words without proper timing or empty words
                        if word and start is not None and end is not None and start < end:
                            # ËΩ¨Êç¢‰∏∫100Á∫≥ÁßíÂçï‰ΩçÔºà‰∏éedge_ttsÂÖºÂÆπÔºâ
                            start_100ns = int(start * 10000000)
                            end_100ns = int(end * 10000000)
                            
                            sub_maker.subs.append(word)
                            sub_maker.offset.append((start_100ns, end_100ns))
                            word_count += 1
                        else:
                            logger.debug(f"Skipping invalid word: '{word}', start: {start}, end: {end}")
            
            logger.info(f"Processed {word_count} word-level timestamps from WhisperX")
        else:
            logger.warning("Skipping word-level processing due to transcription issues")
        
        # Â¶ÇÊûúÊ≤°ÊúâËé∑ÂèñÂà∞ÂçïËØçÁ∫ßÊó∂Èó¥Êà≥ÔºåÂõûÈÄÄÂà∞Âè•Â≠êÁ∫ß (enhanced fallback)
        if not sub_maker.subs or transcription_failed:
            if transcription_failed:
                logger.info("Using sentence-level timing due to poor transcription quality")
            else:
                logger.warning("No word-level timestamps found, falling back to sentence-level")
                
            sentences = utils.split_string_by_punctuations(text)
            audio_duration = len(wav[0]) / 24000  # ÈááÊ†∑Áéá24000Hz
            
            if sentences:
                total_chars = sum(len(s) for s in sentences)
                char_duration = (audio_duration * 10000000) / total_chars if total_chars > 0 else 0
                
                current_offset = 0
                for sentence in sentences:
                    if not sentence.strip():
                        continue
                    
                    sentence_chars = len(sentence)
                    sentence_duration = int(sentence_chars * char_duration)
                    
                    sub_maker.subs.append(sentence.strip())
                    sub_maker.offset.append((current_offset, current_offset + sentence_duration))
                    current_offset += sentence_duration
                    
                logger.info(f"Generated {len(sub_maker.subs)} sentence-level timestamps")
            else:
                # ÊúÄÂêéÁöÑÂõûÈÄÄÊñπÊ°à
                audio_duration_100ns = int(audio_duration * 10000000)
                sub_maker.subs = [text]
                sub_maker.offset = [(0, audio_duration_100ns)]
                logger.info("Using single timestamp for entire text")

        # 5. ËΩ¨Êç¢Èü≥È¢ëÊ†ºÂºè‰∏∫MP3ÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
        final_audio_file = voice_file
        if voice_file.endswith('.mp3'):
            try:
                from moviepy import AudioFileClip
                logger.info("Converting WAV to MP3...")
                audio_clip = AudioFileClip(temp_wav_file)
                audio_clip.write_audiofile(voice_file, logger=None)  # Removed verbose parameter
                audio_clip.close()
                os.remove(temp_wav_file)  # Âà†Èô§‰∏¥Êó∂WAVÊñá‰ª∂
                logger.info("Audio conversion to MP3 completed")
                final_audio_file = voice_file
            except Exception as e:
                logger.warning(f"Failed to convert to MP3, keeping WAV format: {e}")
                # Keep the WAV file with original extension
                final_audio_file = voice_file.replace('.mp3', '.wav')
                os.rename(temp_wav_file, final_audio_file)
                logger.info(f"Saved as WAV: {final_audio_file}")
        else:
            final_audio_file = voice_file
            os.rename(temp_wav_file, voice_file)

        # Log subtitle information for debugging
        if sub_maker.subs:
            logger.info(f"Generated {len(sub_maker.subs)} subtitle entries")
            logger.debug(f"First few subtitle entries: {sub_maker.subs[:5]}")
            logger.debug(f"First few timing offsets: {sub_maker.offset[:5]}")
            
            # Validate subtitle timing
            total_audio_duration = len(wav[0]) / 24000
            last_subtitle_time = sub_maker.offset[-1][1] / 10000000 if sub_maker.offset else 0
            logger.info(f"Audio duration: {total_audio_duration:.2f}s, Last subtitle time: {last_subtitle_time:.2f}s")
            
            # Final quality check
            if transcription_failed:
                logger.warning("‚ö†Ô∏è  Chatterbox TTS transcription had quality issues. Consider:")
                logger.warning("   ‚Ä¢ Using shorter, simpler text")
                logger.warning("   ‚Ä¢ Trying Azure TTS for better accuracy")
                logger.warning("   ‚Ä¢ Using CPU mode (set CHATTERBOX_DEVICE=cpu)")
        else:
            logger.warning("No subtitles generated!")

        logger.success(f"Chatterbox TTS completed with {len(sub_maker.subs)} word/sentence timestamps")
        logger.info(f"Output file: {final_audio_file}")
        
        # Store the actual file path for downstream processing
        sub_maker._actual_audio_file = final_audio_file
        sub_maker._transcription_quality_warning = transcription_failed
        
        return sub_maker

    except Exception as e:
        logger.error(f"Chatterbox TTS failed: {str(e)}")
        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
        temp_wav_file = voice_file.replace('.mp3', '_temp.wav')
        if os.path.exists(temp_wav_file):
            os.remove(temp_wav_file)
        return None


def chatterbox_tts_chunked(
    text: str,
    voice_name: str,
    voice_rate: float,
    voice_file: str,
    voice_volume: float = 1.0,
) -> Union[SubMaker, None]:
    """
    Handle long texts by chunking them into smaller pieces for Chatterbox TTS
    
    This prevents garbled audio that occurs when text is too long
    """
    logger.info("üîÑ Starting chunked Chatterbox TTS processing")
    
    # Split text into optimal chunks
    chunks = chunk_text_for_chatterbox(text, max_chunk_size=300)
    logger.info(f"Split text into {len(chunks)} chunks (max 300 chars each)")
    
    if len(chunks) == 1:
        # If only one chunk, use regular processing
        return chatterbox_tts(chunks[0], voice_name, voice_rate, voice_file, voice_volume)
    
    # Generate audio for each chunk
    temp_audio_files = []
    all_sub_makers = []
    cumulative_duration = 0.0
    
    try:
        for i, chunk in enumerate(chunks):
            logger.info(f"Processing chunk {i+1}/{len(chunks)} ({len(chunk)} chars)")
            
            # Create temporary file for this chunk
            chunk_file = voice_file.replace('.mp3', f'_chunk_{i}.mp3')
            
            # Generate TTS for this chunk
            chunk_result = chatterbox_tts(chunk, voice_name, voice_rate, chunk_file, voice_volume)
            
            if chunk_result:
                chunk_audio_file = getattr(chunk_result, '_actual_audio_file', chunk_file)
                temp_audio_files.append(chunk_audio_file)
                
                # Adjust timing offsets to account for previous chunks
                adjusted_subs = []
                adjusted_offsets = []
                
                cumulative_offset_100ns = int(cumulative_duration * 10000000)
                
                for sub, (start, end) in zip(chunk_result.subs, chunk_result.offset):
                    adjusted_subs.append(sub)
                    adjusted_offsets.append((
                        start + cumulative_offset_100ns,
                        end + cumulative_offset_100ns
                    ))
                
                # Create adjusted SubMaker for this chunk
                adjusted_sub_maker = ensure_submaker_compatibility(SubMaker())
                adjusted_sub_maker.subs = adjusted_subs
                adjusted_sub_maker.offset = adjusted_offsets
                all_sub_makers.append(adjusted_sub_maker)
                
                # Update cumulative duration
                if chunk_result.offset:
                    chunk_duration = chunk_result.offset[-1][1] / 10000000
                    cumulative_duration += chunk_duration
                    
                logger.info(f"Chunk {i+1} completed: {len(chunk_result.subs)} entries, {chunk_duration:.2f}s")
            else:
                logger.error(f"Failed to generate chunk {i+1}")
                return None
        
        # Combine all audio files
        logger.info("üéµ Combining audio chunks...")
        combined_audio = combine_audio_files(temp_audio_files, voice_file)
        
        if combined_audio:
            # Combine all SubMakers
            final_sub_maker = ensure_submaker_compatibility(SubMaker())
            final_sub_maker.subs = []
            final_sub_maker.offset = []
            
            for sub_maker in all_sub_makers:
                final_sub_maker.subs.extend(sub_maker.subs)
                final_sub_maker.offset.extend(sub_maker.offset)
            
            # Set metadata
            final_sub_maker._actual_audio_file = combined_audio
            final_sub_maker._transcription_quality_warning = any(
                getattr(sm, '_transcription_quality_warning', False) for sm in all_sub_makers
            )
            
            logger.success(f"‚úÖ Chunked TTS completed: {len(final_sub_maker.subs)} total entries, {cumulative_duration:.2f}s")
            logger.info(f"Combined audio file: {combined_audio}")
            
            return final_sub_maker
        else:
            logger.error("Failed to combine audio chunks")
            return None
            
    finally:
        # Clean up temporary chunk files
        for temp_file in temp_audio_files:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    logger.debug(f"Cleaned up temporary file: {temp_file}")
                except Exception as e:
                    logger.warning(f"Could not remove temporary file {temp_file}: {e}")


def combine_audio_files(audio_files: list, output_file: str) -> str:
    """
    Combine multiple audio files into a single file
    
    Args:
        audio_files: List of audio file paths to combine
        output_file: Output file path
        
    Returns:
        Path to combined audio file or None if failed
    """
    try:
        from moviepy import AudioFileClip, concatenate_audioclips
        
        logger.info(f"Combining {len(audio_files)} audio files...")
        
        # Load all audio clips
        audio_clips = []
        for audio_file in audio_files:
            if os.path.exists(audio_file):
                clip = AudioFileClip(audio_file)
                audio_clips.append(clip)
                logger.debug(f"Loaded audio clip: {audio_file} ({clip.duration:.2f}s)")
            else:
                logger.warning(f"Audio file not found: {audio_file}")
        
        if not audio_clips:
            logger.error("No valid audio clips to combine")
            return None
        
        # Concatenate all clips
        final_audio = concatenate_audioclips(audio_clips)
        
        # Write combined audio
        final_audio.write_audiofile(output_file, logger=None)
        
        # Clean up clips
        for clip in audio_clips:
            clip.close()
        final_audio.close()
        
        logger.info(f"Successfully combined audio: {output_file} ({final_audio.duration:.2f}s)")
        return output_file
        
    except Exception as e:
        logger.error(f"Failed to combine audio files: {e}")
        return None


def azure_tts_v2(text: str, voice_name: str, voice_file: str) -> Union[SubMaker, None]:
    voice_name = is_azure_v2_voice(voice_name)
    if not voice_name:
        logger.error(f"invalid voice name: {voice_name}")
        raise ValueError(f"invalid voice name: {voice_name}")
    text = text.strip()

    def _format_duration_to_offset(duration) -> int:
        if isinstance(duration, str):
            time_obj = datetime.strptime(duration, "%H:%M:%S.%f")
            milliseconds = (
                (time_obj.hour * 3600000)
                + (time_obj.minute * 60000)
                + (time_obj.second * 1000)
                + (time_obj.microsecond // 1000)
            )
            return milliseconds * 10000

        if isinstance(duration, int):
            return duration

        return 0

    for i in range(3):
        try:
            logger.info(f"start, voice name: {voice_name}, try: {i + 1}")

            import azure.cognitiveservices.speech as speechsdk

            sub_maker = ensure_submaker_compatibility(SubMaker())

            def speech_synthesizer_word_boundary_cb(evt: speechsdk.SessionEventArgs):
                # print('WordBoundary event:')
                # print('\tBoundaryType: {}'.format(evt.boundary_type))
                # print('\tAudioOffset: {}ms'.format((evt.audio_offset + 5000)))
                # print('\tDuration: {}'.format(evt.duration))
                # print('\tText: {}'.format(evt.text))
                # print('\tTextOffset: {}'.format(evt.text_offset))
                # print('\tWordLength: {}'.format(evt.word_length))

                duration = _format_duration_to_offset(str(evt.duration))
                offset = _format_duration_to_offset(evt.audio_offset)
                sub_maker.subs.append(evt.text)
                sub_maker.offset.append((offset, offset + duration))

            # Creates an instance of a speech config with specified subscription key and service region.
            speech_key = config.azure.get("speech_key", "")
            service_region = config.azure.get("speech_region", "")
            if not speech_key or not service_region:
                logger.error("Azure speech key or region is not set")
                return None

            audio_config = speechsdk.audio.AudioOutputConfig(
                filename=voice_file, use_default_speaker=True
            )
            speech_config = speechsdk.SpeechConfig(
                subscription=speech_key, region=service_region
            )
            speech_config.speech_synthesis_voice_name = voice_name
            # speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceResponse_RequestSentenceBoundary,
            #                            value='true')
            speech_config.set_property(
                property_id=speechsdk.PropertyId.SpeechServiceResponse_RequestWordBoundary,
                value="true",
            )

            speech_config.set_speech_synthesis_output_format(
                speechsdk.SpeechSynthesisOutputFormat.Audio48Khz192KBitRateMonoMp3
            )
            speech_synthesizer = speechsdk.SpeechSynthesizer(
                audio_config=audio_config, speech_config=speech_config
            )
            speech_synthesizer.synthesis_word_boundary.connect(
                speech_synthesizer_word_boundary_cb
            )

            result = speech_synthesizer.speak_text_async(text).get()
            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                logger.success(f"azure v2 speech synthesis succeeded: {voice_file}")
                return sub_maker
            elif result.reason == speechsdk.ResultReason.Canceled:
                cancellation_details = result.cancellation_details
                logger.error(
                    f"azure v2 speech synthesis canceled: {cancellation_details.reason}"
                )
                if cancellation_details.reason == speechsdk.CancellationReason.Error:
                    logger.error(
                        f"azure v2 speech synthesis error: {cancellation_details.error_details}"
                    )
            logger.info(f"completed, output file: {voice_file}")
        except Exception as e:
            logger.error(f"failed, error: {str(e)}")
    return None


def _format_text(text: str) -> str:
    # text = text.replace("\n", " ")
    text = text.replace("[", " ")
    text = text.replace("]", " ")
    text = text.replace("(", " ")
    text = text.replace(")", " ")
    text = text.replace("{", " ")
    text = text.replace("}", " ")
    text = text.strip()
    return text


def create_chatterbox_subtitle(sub_maker: SubMaker, text: str, subtitle_file: str):
    """
    Create subtitle file optimized for Chatterbox TTS timestamps
    Handles both word-level and sentence-level timestamps intelligently
    """
    if not sub_maker.subs or not sub_maker.offset:
        logger.warning("No subtitle data available")
        return

    def mktimestamp(seconds: float) -> str:
        """Convert seconds to SRT timestamp format"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

    def formatter(idx: int, start_time: float, end_time: float, sub_text: str) -> str:
        """Format subtitle entry for SRT file"""
        start_t = mktimestamp(start_time)
        end_t = mktimestamp(end_time)
        return f"{idx}\n{start_t} --> {end_t}\n{sub_text}\n"

    try:
        subtitle_entries = []
        subtitle_index = 1

        # Detect if we have word-level or sentence-level timestamps
        avg_sub_length = sum(len(sub) for sub in sub_maker.subs) / len(sub_maker.subs)
        is_word_level = avg_sub_length < 15  # Average word length is typically < 15 chars
        
        if is_word_level:
            logger.info("Processing word-level timestamps for subtitle grouping")
            current_phrase = []
            current_start_time = None
            current_end_time = None

            # Group words into phrases
            for i, (word, (start_100ns, end_100ns)) in enumerate(zip(sub_maker.subs, sub_maker.offset)):
                start_time = start_100ns / 10000000  # Convert to seconds
                end_time = end_100ns / 10000000

                if current_start_time is None:
                    current_start_time = start_time

                current_phrase.append(word)
                current_end_time = end_time

                # End phrase on punctuation or every 8-10 words
                is_punctuation_end = word.rstrip().endswith(('.', '!', '?', '„ÄÇ', 'ÔºÅ', 'Ôºü'))
                is_comma_pause = word.rstrip().endswith((',', 'Ôºå'))
                is_long_phrase = len(current_phrase) >= 8
                is_last_word = i == len(sub_maker.subs) - 1

                if is_punctuation_end or is_long_phrase or is_last_word or (is_comma_pause and len(current_phrase) >= 4):
                    if current_phrase:
                        phrase_text = ' '.join(current_phrase).strip()
                        # Clean up spacing around punctuation
                        phrase_text = re.sub(r'\s+([,.!?„ÄÇÔºåÔºÅÔºü])', r'\1', phrase_text)
                        
                        subtitle_entry = formatter(
                            idx=subtitle_index,
                            start_time=current_start_time,
                            end_time=current_end_time,
                            sub_text=phrase_text
                        )
                        subtitle_entries.append(subtitle_entry)
                        subtitle_index += 1

                    # Reset for next phrase
                    current_phrase = []
                    current_start_time = None
        else:
            logger.info("Processing sentence-level timestamps directly")
            # Use sentence-level timestamps as-is
            for i, (sentence, (start_100ns, end_100ns)) in enumerate(zip(sub_maker.subs, sub_maker.offset)):
                start_time = start_100ns / 10000000
                end_time = end_100ns / 10000000
                
                subtitle_entry = formatter(
                    idx=subtitle_index,
                    start_time=start_time,
                    end_time=end_time,
                    sub_text=sentence.strip()
                )
                subtitle_entries.append(subtitle_entry)
                subtitle_index += 1

        # Write subtitle file
        if subtitle_entries:
            with open(subtitle_file, "w", encoding="utf-8") as file:
                file.write("\n".join(subtitle_entries))
            
            logger.success(f"Chatterbox subtitle file created: {subtitle_file} with {len(subtitle_entries)} entries")
        else:
            logger.warning("No subtitle entries created")

    except Exception as e:
        logger.error(f"Failed to create Chatterbox subtitle: {str(e)}")
        import traceback
        logger.debug(f"Traceback: {traceback.format_exc()}")


def create_subtitle(sub_maker: SubMaker, text: str, subtitle_file: str):
    """
    ‰ºòÂåñÂ≠óÂπïÊñá‰ª∂
    1. Â∞ÜÂ≠óÂπïÊñá‰ª∂ÊåâÁÖßÊ†áÁÇπÁ¨¶Âè∑ÂàÜÂâ≤ÊàêÂ§öË°å
    2. ÈÄêË°åÂåπÈÖçÂ≠óÂπïÊñá‰ª∂‰∏≠ÁöÑÊñáÊú¨
    3. ÁîüÊàêÊñ∞ÁöÑÂ≠óÂπïÊñá‰ª∂
    
    Note: This function is optimized for Azure TTS phrase-level chunks.
    For Chatterbox TTS word-level timestamps, use create_chatterbox_subtitle instead.
    """

    text = _format_text(text)

    def formatter(idx: int, start_time: float, end_time: float, sub_text: str) -> str:
        """
        1
        00:00:00,000 --> 00:00:02,360
        Ë∑ëÊ≠•ÊòØ‰∏ÄÈ°πÁÆÄÂçïÊòìË°åÁöÑËøêÂä®
        """
        start_t = mktimestamp(start_time).replace(".", ",")
        end_t = mktimestamp(end_time).replace(".", ",")
        return f"{idx}\n{start_t} --> {end_t}\n{sub_text}\n"

    start_time = -1.0
    sub_items = []
    sub_index = 0

    script_lines = utils.split_string_by_punctuations(text)

    def match_line(_sub_line: str, _sub_index: int):
        if len(script_lines) <= _sub_index:
            return ""

        _line = script_lines[_sub_index]
        if _sub_line == _line:
            return script_lines[_sub_index].strip()

        _sub_line_ = re.sub(r"[^\w\s]", "", _sub_line)
        _line_ = re.sub(r"[^\w\s]", "", _line)
        if _sub_line_ == _line_:
            return _line_.strip()

        _sub_line_ = re.sub(r"\W+", "", _sub_line)
        _line_ = re.sub(r"\W+", "", _line)
        if _sub_line_ == _line_:
            return _line.strip()

        return ""

    sub_line = ""

    try:
        for _, (offset, sub) in enumerate(zip(sub_maker.offset, sub_maker.subs)):
            _start_time, end_time = offset
            if start_time < 0:
                start_time = _start_time

            sub = unescape(sub)
            sub_line += sub
            sub_text = match_line(sub_line, sub_index)
            if sub_text:
                sub_index += 1
                line = formatter(
                    idx=sub_index,
                    start_time=start_time,
                    end_time=end_time,
                    sub_text=sub_text,
                )
                sub_items.append(line)
                start_time = -1.0
                sub_line = ""

        if len(sub_items) == len(script_lines):
            with open(subtitle_file, "w", encoding="utf-8") as file:
                file.write("\n".join(sub_items) + "\n")
            try:
                sbs = subtitles.file_to_subtitles(subtitle_file, encoding="utf-8")
                duration = max([tb for ((ta, tb), txt) in sbs])
                logger.info(
                    f"completed, subtitle file created: {subtitle_file}, duration: {duration}"
                )
            except Exception as e:
                logger.error(f"failed, error: {str(e)}")
                os.remove(subtitle_file)
        else:
            logger.warning(
                f"failed, sub_items len: {len(sub_items)}, script_lines len: {len(script_lines)}"
            )

    except Exception as e:
        logger.error(f"failed, error: {str(e)}")


def get_audio_duration(sub_maker: SubMaker):
    """
    Ëé∑ÂèñÈü≥È¢ëÊó∂Èïø
    """
    if not sub_maker.offset:
        return 0.0
    return sub_maker.offset[-1][1] / 10000000


# Note: This module contains TTS functions for Azure TTS V1/V2, SiliconFlow TTS, and Chatterbox TTS
# All functions are optimized for production use with proper error handling and fallbacks
